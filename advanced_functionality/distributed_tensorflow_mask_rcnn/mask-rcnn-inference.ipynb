{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask-RCNN Model Inference in Amazon SageMaker\n",
    "\n",
    "This notebook is a step-by-step tutorial on [Mask R-CNN](https://arxiv.org/abs/1703.06870) model inference using [Amazon SageMaker model deployment hosting service](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html).\n",
    "\n",
    "To get started, we initialize an Amazon execution role and initialize a `boto3` session to find our AWS region name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role() # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f'SageMaker Execution Role:{role}')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "aws_region = session.region_name\n",
    "print(f'AWS region:{aws_region}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Push Amazon SageMaker Model Serving Images\n",
    "\n",
    "For this step, the [IAM Role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) attached to this notebook instance needs full access to Amazon ECR service. If you created this notebook instance using the ```./stack-sm.sh``` script in this repository, the IAM Role attached to this notebook instance is already setup with full access to ECR service. \n",
    "\n",
    "Below, we have a choice of two different models for doing inference:\n",
    "\n",
    "1. [TensorPack Faster-RCNN/Mask-RCNN](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN)\n",
    "\n",
    "2. [AWS Samples Mask R-CNN](https://github.com/aws-samples/mask-rcnn-tensorflow)\n",
    "\n",
    "It is recommended that you build and push both Amazon SageMaker model <b>serving</b> container images below and use either image for serving the model for inference later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorPack Faster-RCNN/Mask-RCNN Model Serving Image\n",
    "\n",
    "Use ```./container-serving/build_tools/build_and_push.sh``` script to build and push the TensorPack Faster-RCNN/Mask-RCNN <b>serving</b> image to Amazon ECR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./container-serving/build_tools/build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your *AWS region* as argument, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! ./container-serving/build_tools/build_and_push.sh {aws_region}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set ```tensorpack_image``` below to Amazon ECR URI of the <b>serving</b> image you pushed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorpack_image = '807253771232.dkr.ecr.us-east-1.amazonaws.com/mask-rcnn-tensorpack-serving-sagemaker:tf1.13-tp26664c3' #<amazon-ecr-uri>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Samples Mask R-CNN Model Serving Image\n",
    "Use ```./container-serving-optimized/build_tools/build_and_push.sh``` script to build and push the AWS Samples Mask R-CNN <b>serving</b> image to Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./container-serving-optimized/build_tools/build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your *AWS region* as argument, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! ./container-serving-optimized/build_tools/build_and_push.sh {aws_region}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Set ```aws_samples_image``` below to Amazon ECR URI of the <b>serving</b> image you pushed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_samples_image = '807253771232.dkr.ecr.us-east-1.amazonaws.com/mask-rcnn-tensorflow-serving-sagemaker:tf1.13-153442b' #<amazon-ecr-uri>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Initialization \n",
    "We have built and pushed the <b>serving</b> images to Amazon ECR. Now we are ready to start using Amazon SageMaker for deploying model endpoint for inference. Next, we create a SageMaker sessiom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.session.Session(boto_session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set ```serving_image``` to either the `tensorpack_image` or the `aws_samples_image` variable you defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_image =  tensorpack_image # set to tensorpack_image or aws_samples_image \n",
    "print(f'serving image: {serving_image}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Amazon SageMaker Model\n",
    "Next, we define Amazon SageMaker model that we will use to host the model on an Amazon SageMaker endpoint. The `model_url` below must point to a trained model. Please esnure that the trained model is consistent with the model serving image used for deploying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = 's3://aws-ajayvohra-ml-data/mask-rcnn/sagemaker/output/mask-rcnn-s3-2020-02-06-18-12-32-906/output/model.tar.gz'\n",
    "serving_container = {\n",
    "    'Image': serving_image,\n",
    "    'ModelDataUrl': model_url,\n",
    "    'Mode': 'SingleModel',\n",
    "    'Environment': { 'SM_MODEL_DIR' : '/opt/ml/model' }\n",
    "}\n",
    "\n",
    "create_model_response = sagemaker_session.create_model(name=\"mask-rcnn-model\", \n",
    "                                                       role=role, \n",
    "                                                       container_defs=serving_container)\n",
    "\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the Amazon SageMaker model hosted service endpoint configuration. Below we use a 1 instance of `ml.p3.2xlarge` (1 GPU) instance type to serve the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epc = sagemaker_session.create_endpoint_config(\n",
    "    name=\"mask-rcnn-model-endpoint-config\", \n",
    "    model_name=\"mask-rcnn-model\", \n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.p3.2xlarge')\n",
    "print(epc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify the Amazon SageMaker endpoint name for the endpoint used to serve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name='mask-rcnn-model-endpoint'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the Amazon SageMaker endpoint using the endpoint configuration we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep=sagemaker_session.create_endpoint(endpoint_name=endpoint_name, config_name=epc, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Amazon SageMaker endpoint is in service, we will use the endpoint to do inference for test images. Below we download an example image from the web and use it as input for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import requests\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "url=\"https://i.kinja-img.com/gawker-media/image/upload/c_scale,f_auto,fl_progressive,q_80,w_1600/ekpvcpwo560egadg2mvm.jpg\"\n",
    "\n",
    "img_id=\"image1.jpg\"\n",
    "img_local_path=os.path.join(\"/tmp\", img_id)\n",
    "if os.path.isfile(img_local_path):\n",
    "    print(f\"Removing existing tmp file:{img_local_path}\")\n",
    "    os.remove(img_local_path)\n",
    "    \n",
    "with urllib.request.urlopen(url) as url_stream:\n",
    "    with open(img_local_path, 'wb') as f:\n",
    "        f.write(url_stream.read())\n",
    "\n",
    "img=cv2.imread(img_local_path, cv2.IMREAD_COLOR)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show the image we downloaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(img.shape[1]//50, img.shape[0]//50))\n",
    "ax.imshow(img.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we invoke the Amazon SageMaker endpoint to detect objects in the image using the model we deployed to the endpoint above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import base64\n",
    "import json\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "with open(img_local_path, \"rb\") as image_file:\n",
    "    img_data = base64.b64encode(image_file.read())\n",
    "    data = {\"img_id\": img_id}\n",
    "    data[\"img_data\"] = img_data.decode('utf-8')\n",
    "    body=json.dumps(data).encode('utf-8')\n",
    "    \n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType=\"application/json\",\n",
    "                                  Accept=\"application/json\",\n",
    "                                  Body=body)\n",
    "body=response['Body'].read()\n",
    "msg=body.decode('utf-8')\n",
    "data=json.loads(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from the endpoint includes annotations for the detected objects in COCO annotations format. \n",
    "\n",
    "Below we define a function to convert COCO run length encoding for image mask to a binary image mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rle_to_binary_mask(rle, img_shape):\n",
    "    value = 0\n",
    "    mask_array = []\n",
    "    for count in rle:\n",
    "        mask_array.extend([int(value)]*count)\n",
    "        value = (value + 1) % 2\n",
    "    \n",
    "    assert len(mask_array) == img_shape[0]*img_shape[1]\n",
    "    b_mask = np.array(mask_array, dtype=np.uint8).reshape(img_shape)\n",
    "    \n",
    "    return b_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a function for generating random colors for visualizing annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "import random\n",
    "\n",
    "def random_colors(N, bright=False):\n",
    "    brightness = 1.0 if bright else 0.7\n",
    "    hsv = [(i / N, 1, brightness) for i in range(N)]\n",
    "    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
    "    random.shuffle(colors)\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a function to apply a binary mask for an annotation to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_binary_mask(image, mask, color, alpha=0.5):\n",
    "    a_mask = np.stack([mask]*3, axis=2).astype(np.int8)\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = np.where(mask == 1, image[:, :, c] *(1 - alpha) + alpha * color[c]*255,image[:, :, c])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we process all the images and apply the masks in the annotations. Each annotation also includes bounding box, cateory id  and cateogry class name, beside the segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = data['annotations']\n",
    "colors = random_colors(len(annotations))\n",
    "for i, a in enumerate(annotations):\n",
    "    segm = a['segmentation']\n",
    "    img_shape = tuple(segm['size'])\n",
    "    rle = segm['counts']\n",
    "    b_mask = rle_to_binary_mask(rle, img_shape)\n",
    "    img = apply_binary_mask(img, b_mask, colors[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we visualize the image with the applied segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(img.shape[1]//50, img.shape[0]//50))\n",
    "ax.imshow(img.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
